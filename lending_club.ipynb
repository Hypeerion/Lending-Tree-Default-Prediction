{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lending Club\n",
    "\n",
    "### A complete data analysis project on loans issued by Lending Club\n",
    "\n",
    "Lending Club is a website that allows investors to pool their money to issue loans to borrowers, with Lending Club acting as an intermediary on the transactions.  Interest rates are set by the borrowers credit score as well as the proprietary data analytics of Lending club.\n",
    "\n",
    "Included in this folder is a data dictionary describing the data set.  The data dictionary is in XLS (Excel) format, and has a few tabs contained within.  The first tab is called \"LoanStats\" and only contains information on loans that were issued, ignoring all loans that were rejected.  This will include loans that defaulted, since they were issued in the first place, and current loans, in addition to loans that were satisfied by the borrower.\n",
    "\n",
    "We will be attempting to build a machine learning model that can accurately predict whether a borrower will pay back their loan in a timely fashion. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features and Labels\n",
    "\n",
    "Before we build our model, we will need to identify any relevant columns of data that can be included in our feature-set for predictive power.  \n",
    "\n",
    "We will also need to identify a \"label\" or what our model is *going to attempt to predict*.\n",
    "\n",
    "We will only be using a calendar-range subset of the data from 2007-2011.  The reason for this is that loans made during that time period have mostly ended, so we will have a good idea how many of them have defaulted and how many have been satisfactorily paid. \n",
    "\n",
    "Let's take a look at the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>member_id</th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>funded_amnt</th>\n",
       "      <th>funded_amnt_inv</th>\n",
       "      <th>term</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>installment</th>\n",
       "      <th>grade</th>\n",
       "      <th>sub_grade</th>\n",
       "      <th>...</th>\n",
       "      <th>hardship_payoff_balance_amount</th>\n",
       "      <th>hardship_last_payment_amount</th>\n",
       "      <th>disbursement_method</th>\n",
       "      <th>debt_settlement_flag</th>\n",
       "      <th>debt_settlement_flag_date</th>\n",
       "      <th>settlement_status</th>\n",
       "      <th>settlement_date</th>\n",
       "      <th>settlement_amount</th>\n",
       "      <th>settlement_percentage</th>\n",
       "      <th>settlement_term</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>68407277</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>13.99</td>\n",
       "      <td>123.03</td>\n",
       "      <td>C</td>\n",
       "      <td>C4</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>68355089</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24700.0</td>\n",
       "      <td>24700.0</td>\n",
       "      <td>24700.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>11.99</td>\n",
       "      <td>820.28</td>\n",
       "      <td>C</td>\n",
       "      <td>C1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68341763</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>60 months</td>\n",
       "      <td>10.78</td>\n",
       "      <td>432.66</td>\n",
       "      <td>B</td>\n",
       "      <td>B4</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>66310712</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35000.0</td>\n",
       "      <td>35000.0</td>\n",
       "      <td>35000.0</td>\n",
       "      <td>60 months</td>\n",
       "      <td>14.85</td>\n",
       "      <td>829.90</td>\n",
       "      <td>C</td>\n",
       "      <td>C5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>68476807</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10400.0</td>\n",
       "      <td>10400.0</td>\n",
       "      <td>10400.0</td>\n",
       "      <td>60 months</td>\n",
       "      <td>22.45</td>\n",
       "      <td>289.91</td>\n",
       "      <td>F</td>\n",
       "      <td>F1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 151 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  member_id  loan_amnt  funded_amnt  funded_amnt_inv        term  \\\n",
       "0  68407277        NaN     3600.0       3600.0           3600.0   36 months   \n",
       "1  68355089        NaN    24700.0      24700.0          24700.0   36 months   \n",
       "2  68341763        NaN    20000.0      20000.0          20000.0   60 months   \n",
       "3  66310712        NaN    35000.0      35000.0          35000.0   60 months   \n",
       "4  68476807        NaN    10400.0      10400.0          10400.0   60 months   \n",
       "\n",
       "   int_rate  installment grade sub_grade  ... hardship_payoff_balance_amount  \\\n",
       "0     13.99       123.03     C        C4  ...                            NaN   \n",
       "1     11.99       820.28     C        C1  ...                            NaN   \n",
       "2     10.78       432.66     B        B4  ...                            NaN   \n",
       "3     14.85       829.90     C        C5  ...                            NaN   \n",
       "4     22.45       289.91     F        F1  ...                            NaN   \n",
       "\n",
       "  hardship_last_payment_amount disbursement_method  debt_settlement_flag  \\\n",
       "0                          NaN                Cash                     N   \n",
       "1                          NaN                Cash                     N   \n",
       "2                          NaN                Cash                     N   \n",
       "3                          NaN                Cash                     N   \n",
       "4                          NaN                Cash                     N   \n",
       "\n",
       "  debt_settlement_flag_date settlement_status settlement_date  \\\n",
       "0                       NaN               NaN             NaN   \n",
       "1                       NaN               NaN             NaN   \n",
       "2                       NaN               NaN             NaN   \n",
       "3                       NaN               NaN             NaN   \n",
       "4                       NaN               NaN             NaN   \n",
       "\n",
       "  settlement_amount settlement_percentage settlement_term  \n",
       "0               NaN                   NaN             NaN  \n",
       "1               NaN                   NaN             NaN  \n",
       "2               NaN                   NaN             NaN  \n",
       "3               NaN                   NaN             NaN  \n",
       "4               NaN                   NaN             NaN  \n",
       "\n",
       "[5 rows x 151 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('loanstats.csv', low_memory=False)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id :  object\n",
      "member_id :  float64\n",
      "loan_amnt :  float64\n",
      "funded_amnt :  float64\n",
      "funded_amnt_inv :  float64\n",
      "term :  object\n",
      "int_rate :  float64\n",
      "installment :  float64\n",
      "grade :  object\n",
      "sub_grade :  object\n",
      "emp_title :  object\n",
      "emp_length :  object\n",
      "home_ownership :  object\n",
      "annual_inc :  float64\n",
      "verification_status :  object\n",
      "issue_d :  object\n",
      "loan_status :  object\n",
      "pymnt_plan :  object\n",
      "url :  object\n",
      "desc :  object\n",
      "purpose :  object\n",
      "title :  object\n",
      "zip_code :  object\n",
      "addr_state :  object\n",
      "dti :  float64\n",
      "delinq_2yrs :  float64\n",
      "earliest_cr_line :  object\n",
      "fico_range_low :  float64\n",
      "fico_range_high :  float64\n",
      "inq_last_6mths :  float64\n",
      "mths_since_last_delinq :  float64\n",
      "mths_since_last_record :  float64\n",
      "open_acc :  float64\n",
      "pub_rec :  float64\n",
      "revol_bal :  float64\n",
      "revol_util :  float64\n",
      "total_acc :  float64\n",
      "initial_list_status :  object\n",
      "out_prncp :  float64\n",
      "out_prncp_inv :  float64\n",
      "total_pymnt :  float64\n",
      "total_pymnt_inv :  float64\n",
      "total_rec_prncp :  float64\n",
      "total_rec_int :  float64\n",
      "total_rec_late_fee :  float64\n",
      "recoveries :  float64\n",
      "collection_recovery_fee :  float64\n",
      "last_pymnt_d :  object\n",
      "last_pymnt_amnt :  float64\n",
      "next_pymnt_d :  object\n",
      "last_credit_pull_d :  object\n",
      "last_fico_range_high :  float64\n",
      "last_fico_range_low :  float64\n",
      "collections_12_mths_ex_med :  float64\n",
      "mths_since_last_major_derog :  float64\n",
      "policy_code :  float64\n",
      "application_type :  object\n",
      "annual_inc_joint :  float64\n",
      "dti_joint :  float64\n",
      "verification_status_joint :  object\n",
      "acc_now_delinq :  float64\n",
      "tot_coll_amt :  float64\n",
      "tot_cur_bal :  float64\n",
      "open_acc_6m :  float64\n",
      "open_act_il :  float64\n",
      "open_il_12m :  float64\n",
      "open_il_24m :  float64\n",
      "mths_since_rcnt_il :  float64\n",
      "total_bal_il :  float64\n",
      "il_util :  float64\n",
      "open_rv_12m :  float64\n",
      "open_rv_24m :  float64\n",
      "max_bal_bc :  float64\n",
      "all_util :  float64\n",
      "total_rev_hi_lim :  float64\n",
      "inq_fi :  float64\n",
      "total_cu_tl :  float64\n",
      "inq_last_12m :  float64\n",
      "acc_open_past_24mths :  float64\n",
      "avg_cur_bal :  float64\n",
      "bc_open_to_buy :  float64\n",
      "bc_util :  float64\n",
      "chargeoff_within_12_mths :  float64\n",
      "delinq_amnt :  float64\n",
      "mo_sin_old_il_acct :  float64\n",
      "mo_sin_old_rev_tl_op :  float64\n",
      "mo_sin_rcnt_rev_tl_op :  float64\n",
      "mo_sin_rcnt_tl :  float64\n",
      "mort_acc :  float64\n",
      "mths_since_recent_bc :  float64\n",
      "mths_since_recent_bc_dlq :  float64\n",
      "mths_since_recent_inq :  float64\n",
      "mths_since_recent_revol_delinq :  float64\n",
      "num_accts_ever_120_pd :  float64\n",
      "num_actv_bc_tl :  float64\n",
      "num_actv_rev_tl :  float64\n",
      "num_bc_sats :  float64\n",
      "num_bc_tl :  float64\n",
      "num_il_tl :  float64\n",
      "num_op_rev_tl :  float64\n",
      "num_rev_accts :  float64\n",
      "num_rev_tl_bal_gt_0 :  float64\n",
      "num_sats :  float64\n",
      "num_tl_120dpd_2m :  float64\n",
      "num_tl_30dpd :  float64\n",
      "num_tl_90g_dpd_24m :  float64\n",
      "num_tl_op_past_12m :  float64\n",
      "pct_tl_nvr_dlq :  float64\n",
      "percent_bc_gt_75 :  float64\n",
      "pub_rec_bankruptcies :  float64\n",
      "tax_liens :  float64\n",
      "tot_hi_cred_lim :  float64\n",
      "total_bal_ex_mort :  float64\n",
      "total_bc_limit :  float64\n",
      "total_il_high_credit_limit :  float64\n",
      "revol_bal_joint :  float64\n",
      "sec_app_fico_range_low :  float64\n",
      "sec_app_fico_range_high :  float64\n",
      "sec_app_earliest_cr_line :  object\n",
      "sec_app_inq_last_6mths :  float64\n",
      "sec_app_mort_acc :  float64\n",
      "sec_app_open_acc :  float64\n",
      "sec_app_revol_util :  float64\n",
      "sec_app_open_act_il :  float64\n",
      "sec_app_num_rev_accts :  float64\n",
      "sec_app_chargeoff_within_12_mths :  float64\n",
      "sec_app_collections_12_mths_ex_med :  float64\n",
      "sec_app_mths_since_last_major_derog :  float64\n",
      "hardship_flag :  object\n",
      "hardship_type :  object\n",
      "hardship_reason :  object\n",
      "hardship_status :  object\n",
      "deferral_term :  float64\n",
      "hardship_amount :  float64\n",
      "hardship_start_date :  object\n",
      "hardship_end_date :  object\n",
      "payment_plan_start_date :  object\n",
      "hardship_length :  float64\n",
      "hardship_dpd :  float64\n",
      "hardship_loan_status :  object\n",
      "orig_projected_additional_accrued_interest :  float64\n",
      "hardship_payoff_balance_amount :  float64\n",
      "hardship_last_payment_amount :  float64\n",
      "disbursement_method :  object\n",
      "debt_settlement_flag :  object\n",
      "debt_settlement_flag_date :  object\n",
      "settlement_status :  object\n",
      "settlement_date :  object\n",
      "settlement_amount :  float64\n",
      "settlement_percentage :  float64\n",
      "settlement_term :  float64\n"
     ]
    }
   ],
   "source": [
    "# printing columns\n",
    "for col in df.columns:\n",
    "    print(col,\": \", df[col].dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2260701, 151)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set is large, so we will eliminate all columns that are missing 50% or greater values.\n",
    "\n",
    "We will remove the 'desc' column which contains a lengthy description of the reason the borrower requested the loan.\n",
    "\n",
    "We will also remove the 'url' column which contains a link that investors backing the loan can use to check up on its progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['member_id',\n",
       " 'desc',\n",
       " 'mths_since_last_delinq',\n",
       " 'mths_since_last_record',\n",
       " 'next_pymnt_d',\n",
       " 'mths_since_last_major_derog',\n",
       " 'annual_inc_joint',\n",
       " 'dti_joint',\n",
       " 'verification_status_joint',\n",
       " 'mths_since_recent_bc_dlq',\n",
       " 'mths_since_recent_revol_delinq',\n",
       " 'revol_bal_joint',\n",
       " 'sec_app_fico_range_low',\n",
       " 'sec_app_fico_range_high',\n",
       " 'sec_app_earliest_cr_line',\n",
       " 'sec_app_inq_last_6mths',\n",
       " 'sec_app_mort_acc',\n",
       " 'sec_app_open_acc',\n",
       " 'sec_app_revol_util',\n",
       " 'sec_app_open_act_il',\n",
       " 'sec_app_num_rev_accts',\n",
       " 'sec_app_chargeoff_within_12_mths',\n",
       " 'sec_app_collections_12_mths_ex_med',\n",
       " 'sec_app_mths_since_last_major_derog',\n",
       " 'hardship_type',\n",
       " 'hardship_reason',\n",
       " 'hardship_status',\n",
       " 'deferral_term',\n",
       " 'hardship_amount',\n",
       " 'hardship_start_date',\n",
       " 'hardship_end_date',\n",
       " 'payment_plan_start_date',\n",
       " 'hardship_length',\n",
       " 'hardship_dpd',\n",
       " 'hardship_loan_status',\n",
       " 'orig_projected_additional_accrued_interest',\n",
       " 'hardship_payoff_balance_amount',\n",
       " 'hardship_last_payment_amount',\n",
       " 'debt_settlement_flag_date',\n",
       " 'settlement_status',\n",
       " 'settlement_date',\n",
       " 'settlement_amount',\n",
       " 'settlement_percentage',\n",
       " 'settlement_term']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# null values of each column\n",
    "null_counts = df.isnull().sum()\n",
    "\n",
    "# amount of rows in df\n",
    "df_rows = len(df)\n",
    "\n",
    "# list to hold removed columns\n",
    "remove_cols = []\n",
    "\n",
    "# loop to remove any columns with 50% or greater missing values\n",
    "for col in df.columns:\n",
    "    if null_counts[col] / df_rows >= .5:\n",
    "        df.drop(col, axis=1, inplace=True)\n",
    "        remove_cols.append(col)\n",
    "        \n",
    "        \n",
    "# print all columns removed\n",
    "remove_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing datetime module\n",
    "import datetime as dt\n",
    "\n",
    "# converting dates to datetime objects\n",
    "df['issue_d']= pd.to_datetime(df.issue_d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing all loans that start after 2011\n",
    "\n",
    "df = df[df.issue_d <= dt.datetime(2011,12,31)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking to make sure our change took place\n",
    "\n",
    "print('min: ',df.issue_d.min())\n",
    "print('max: ',df.issue_d.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing 'url' columns ( 'desc' was removed because it contained too many missing values)\n",
    "\n",
    "df.drop('url', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing More Columns From Data\n",
    "\n",
    "Because this data set contains so many columns, we are going to examine the first 18 columns and see if there are any that we can eliminate from our feature set, or choose as our label.\n",
    "\n",
    "We can eliminate columns that:\n",
    "\n",
    "- Leak data, or any information about a loan that was collected after the loan was given\n",
    "\n",
    "- Have no effect on a borrowers ability to pay back the loan\n",
    "\n",
    "- Require additional cleaning or processing to be useful as a feature\n",
    "\n",
    "- Are duplicates or near-duplicates of other columns\n",
    "\n",
    "### Here are the first 18 rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_18 = df.columns[0:18]\n",
    "second_18 = df.columns[18:36]\n",
    "third_18 = df.columns[36:54]\n",
    "\n",
    "for col in first_18:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "id: remove due to it being arbitrary\n",
    "\n",
    "funded_amnt: removed because the loan has already been funded at this point\n",
    "\n",
    "funded_amnt_inv: removed because the loan has already been funded at this point\n",
    "\n",
    "grade: duplicate info of interest rate, with less granularity\n",
    "\n",
    "sub_grade: duplicate info of interest rate, with less granularity\n",
    "\n",
    "emp_title: would need to be categorized manually, which would take too much work on a large data set\n",
    "\n",
    "issue_d: removed because the loan has already been funded at this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing columns\n",
    "\n",
    "df.drop(['id',\n",
    "         'funded_amnt',\n",
    "         'funded_amnt_inv',\n",
    "         'grade','sub_grade',\n",
    "         'emp_title',\n",
    "         'issue_d'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here are the next 18 rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in second_18:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "zip_code: only contains the first thee numbers, so dropped\n",
    "\n",
    "out_prncp: removed because the loan has already been funded at this point \n",
    "\n",
    "out_prncp_inv: removed because the loan has already been funded at this point\n",
    "\n",
    "total_pymnt: removed because the loan has already been funded at this point\n",
    "\n",
    "total_pymnt_inv: removed because the loan has already been funded at this point\n",
    "\n",
    "total_rec_prncp: removed because the loan has already been funded at this point\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing columns\n",
    "\n",
    "df.drop(['zip_code',\n",
    "         'out_prncp',\n",
    "         'out_prncp_inv',\n",
    "         'total_pymnt',\n",
    "         'total_pymnt_inv',\n",
    "         'total_rec_prncp'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Here are the next 18 rows:\n",
    "\n",
    "for col in third_18:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "total_rec_int: removed because the loan has already been funded at this point \n",
    "\n",
    "total_rec_late_fee: removed because the loan has already been funded at this point\n",
    "\n",
    "recoveries: removed because the loan has already been funded at this point\n",
    "\n",
    "collection_recovery_fee: removed because the loan has already been funded at this point\n",
    "\n",
    "last_pymnt_d: removed because the loan has already been funded at this point\n",
    "\n",
    "last_pymnt_amnt: removed because the loan has already been funded at this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing columns\n",
    "\n",
    "df.drop(['total_rec_int',\n",
    "         'total_rec_late_fee',\n",
    "         'recoveries',\n",
    "         'collection_recovery_fee',\n",
    "         'last_pymnt_d',\n",
    "         'last_pymnt_amnt'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, from our remaining columns, we will choose a subset that look interesting for predictions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['loan_amnt', 'term', 'int_rate',\n",
    "        'installment', 'emp_length', 'home_ownership',\n",
    "        'annual_inc', 'verification_status', 'loan_status',\n",
    "        'pymnt_plan', 'purpose', 'title',\n",
    "        'addr_state', 'dti', 'delinq_2yrs',\n",
    "        'earliest_cr_line', 'inq_last_6mths', 'open_acc',\n",
    "        'pub_rec', 'revol_bal', 'revol_util',\n",
    "        'total_acc', 'initial_list_status', 'last_credit_pull_d',\n",
    "        'collections_12_mths_ex_med', 'policy_code', 'application_type',\n",
    "        'chargeoff_within_12_mths', 'pub_rec_bankruptcies', 'tax_liens']]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing Our Label\n",
    "\n",
    "'loan_status' seems to be an ideal candidate for a columns that we could predict.  Here are the possible values for that column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# values and counts for each values in 'loan status'\n",
    "\n",
    "df.loan_status.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the Lending Tree website, we can see an explanation for each possible value:\n",
    "\n",
    "- <b>Fully Paid</b> - The loan is paid off\n",
    "\n",
    "- <b>Charged Off</b>: The loan defaulted\n",
    "\n",
    "- <b>Does not meet the credit policy. Status:Fully Paid:</b> Even though the loan is paid off, the borrower would not meet the criteria for borrowing at the end of the term of his loan\n",
    "\n",
    "- <b>Does not meet the credit policy. Status:Charged Off:</b> The loan defaulted, and when it did, the borrower did not meet the criteria for borrowing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are only interested in whether the loan was paid off or not, we will have to change the values of the latter two possible values, since they contain additional, unneeded information.\n",
    "\n",
    "We will also change the value of any paid off loan to a 1, and a charged off loan to a 2.  This way, we have turned our labels into binary values, allowing our model to simply solve a binary classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating mapping dictionary to replace all values with 1's or 0's\n",
    "mapping_dict = {'loan_status': {\n",
    "                   'Does not meet the credit policy. Status:Fully Paid':1,\n",
    "                   'Does not meet the credit policy. Status:Charged Off':0,\n",
    "                   'Fully Paid':1,\n",
    "                   'Charged Off':0}\n",
    "               }\n",
    "\n",
    "# mapping onto our datafram\n",
    "df = df.replace(mapping_dict)\n",
    "\n",
    "# testing to make sure the results took place\n",
    "df.loan_status.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminating Single-Value Columns\n",
    "\n",
    "Any column that doesn't contain more than a single value cannot be used to help us predict anything.\n",
    "Now we well check each column to make sure there is variation in its data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list to hold single value columns\n",
    "single_val_cols = []\n",
    "\n",
    "# checking each column for number of values in its \"value_counts\" function\n",
    "for col in df.columns:\n",
    "    if len(df[col].value_counts()) == 1:\n",
    "        single_val_cols.append(col)\n",
    "        \n",
    "        \n",
    "print(single_val_cols)\n",
    "\n",
    "# dropping those columns\n",
    "df.drop(single_val_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "Now that we have removed all unnecessary columns and selected a grouping to work with as our features, we will need to thoroughly clean the data.  This will involve tasks such as:\n",
    "\n",
    "- removing rows/columns with missing data or imputing the missing data\n",
    "\n",
    "\n",
    "- converting columns to data types we can work with numerically or categorically\n",
    "\n",
    "\n",
    "- impute new feature columns from processing and combinations of existing columns\n",
    "\n",
    "\n",
    "- removing any other columns we've chosen that are hard to work with, or won't work as a feature for whatever reason"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values\n",
    "\n",
    "First, let's see how many missing values are left in our data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print null values of each columns that have null values\n",
    "\n",
    "null_col_sums = df.isnull().sum()\n",
    "null_col_sums[null_col_sums>0].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print percentage of null values of each columns that have null values\n",
    "\n",
    "null_col_sums = df.isnull().sum() / len(df)\n",
    "null_col_sums[null_col_sums>0].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will examine our two columns that have the largest number of missing values\n",
    "\n",
    "<b>pub_rec_bankruptcies<b/> and <b>emp_length<b/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observing the frequency of values in pub_rec_bankruptcies\n",
    "\n",
    "df.pub_rec_bankruptcies.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the variation is low in this column, domain knowledge tells us that knowing if someone has declared bankruptcy could be a useful value in assessing their credit history.   We will impute the much more common value of \"0\" (did not have bankruptcy) into the missing values.\n",
    "\n",
    "We will also impute \"0\" into the eight \"2.0\" values, to allow this column to be a binary.  With extremely little accuracy loss, we can avoid splitting up the column into dummies or drop eight rows of data from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputing missing values with \"0\"\n",
    "\n",
    "df.pub_rec_bankruptcies.loc[df.pub_rec_bankruptcies.isnull()] = 0\n",
    "\n",
    "\n",
    "# imputing \"0\" into values with \"2.0\"\n",
    "\n",
    "df.pub_rec_bankruptcies.loc[df.pub_rec_bankruptcies == 2.0] = 0\n",
    "\n",
    "\n",
    "\n",
    "# Testing to make sure our column is now a binary \n",
    "\n",
    "df.pub_rec_bankruptcies.value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other column missing data is <b>emp_length</b>.\n",
    "    \n",
    "emp_length is a representation of employment length for the borrower.  From domain knowledge, we know the stability of a borrowers job will greatly affect his ability to pay back a loan.  Let's examine the values of this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.emp_length.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing the Rest of the Missing Data\n",
    "\n",
    "There is much variation and only ~2% missing values.  For this columns, we will drop all rows missing this data.  Since all the rest of the columns barely have missing values at all, we will go ahead and remove all rows with missing data from the set at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping rows missing emp_length\n",
    "\n",
    "df.dropna(axis=0, inplace=True)\n",
    "\n",
    "\n",
    "# testing to make sure there are no more missing values\n",
    "\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observing the Data From Each Data Type In Remaining Columns\n",
    "\n",
    "To see if we need to make adjustments to any of the columns, we will break each data type into a group to observe the different values contained within.\n",
    "\n",
    "### Object Data Types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# creating dataframe of object type columns\n",
    "object_cols = df.select_dtypes(include='object')\n",
    "\n",
    "#printing first row\n",
    "object_cols.iloc[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These columns may need to be made categorical, but first we will have to look into their value_counts to see how many categories each has.  We may need to invent categories by processing the data if necessary.\n",
    "\n",
    "- home_ownership - Will need to be made categorical\n",
    "\n",
    "- verification_status: Indicates whether the borrowing was verified by Lending Tree\n",
    "\n",
    "- emp_length -  This column will need to be numerical\n",
    "\n",
    "- term - This column will need to be turned into a categorical column\n",
    "\n",
    "- addr_state: State of the borrower\n",
    "\n",
    "- purpose: Provided by the homeowner (reason for the loan)\n",
    "\n",
    "- title: Loan title from borrower\n",
    "\n",
    "Columns that seem to require too much work to be made useful:\n",
    "\n",
    "- earliest_cr_line: We will remove this\n",
    "\n",
    "- last_credit_pull_d: We will remove this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping earliest_cr_line and last_credit_pull_d\n",
    "df.drop(['last_credit_pull_d', 'earliest_cr_line'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looping over the remaining cols and printing their value counts\n",
    "for col in df.select_dtypes(include='object'):\n",
    "    print(df[col].value_counts())\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- The home_ownership, verification_status, emp_length, and term columns all contain only a few distinct values, so we will convert them to dummy columns to use as categorical features\n",
    "\n",
    "- The emp_length columns will have to be cleaned first, removing the text before turning it into dummy columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning the emp_length column\n",
    "\n",
    "# creating dictionary to replace all values\n",
    "\n",
    "mapping_dict = {'emp_length':{\n",
    "                \"10+ years\": 10,\n",
    "                \"9 years\": 9,\n",
    "                \"8 years\": 8,\n",
    "                \"7 years\": 7,\n",
    "                \"6 years\": 6,\n",
    "                \"5 years\": 5,\n",
    "                \"4 years\": 4,\n",
    "                \"3 years\": 3,\n",
    "                \"2 years\": 2,\n",
    "                \"1 year\": 1,\n",
    "                \"< 1 year\": 0,\n",
    "                \"n/a\": 0}\n",
    "               }\n",
    "\n",
    "# replacing values in df\n",
    "df = df.replace(mapping_dict)\n",
    "\n",
    "\n",
    "# testing to make sure it took\n",
    "df.emp_length.value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The addr_state column contains too many discrete values, since it represents the State the borrower lives in.  This would create ~50 different dummy columns, so we will drop this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping addr_state\n",
    "df.drop('addr_state', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The title and purpose columns contain similar data, but the purpose columns contains fewer discrete values, so we will drop the title column as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping title column\n",
    "df.drop('title', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can create dummy columns for the remaining home_ownership, verification_status, purpose, and term columns, which will create a single column with 0's or 1's for each category within the column.\n",
    "\n",
    "Then, once created, we can concatenate the dummy columns back to the main dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dummy_cols\n",
    "dummy_cols = pd.get_dummies(df[['home_ownership', 'verification_status', 'purpose', 'term']])\n",
    "\n",
    "# concatenating dummy_cols to main df\n",
    "df = pd.concat([df, dummy_cols], axis=1)\n",
    "\n",
    "# checking to make sure it worked\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the dummy columns appended to the end of our dataframe, we can delete the original columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping original columns we turned into dummy columns\n",
    "\n",
    "df.drop(['home_ownership', 'verification_status', 'purpose', 'term'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking to make sure new dummy columns appended and old columns were deleted.\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing A Metric For Errors In Our Prediction\n",
    "\n",
    "We will run into a problem since many more loans are marked as paid than unpaid in our loan_status column.  This is called a \"class imbalance\" and can throw off machine learning algorithms.  The reason for this is the algorithm can have a very high \"success rate\" simply by predicting that a loan will be paid off.\n",
    "\n",
    "We can remedy this situation by analyzing the number of:\n",
    "\n",
    " - False Negatives: Loans we predict will default and actually are paid off.  And...\n",
    "\n",
    "\n",
    " - False Positives: Loans we predict will be paid off, but instead fall into default.\n",
    "\n",
    "My minimizing the number of False Negatives and minimizing the number of False Positives, we establish a better metric for how profitable our predictions will be on the production line.\n",
    "\n",
    "If we have a False Negative, we lose money by not profiting on a loan we should have underwritten.\n",
    "\n",
    "If we have a False Positive, we lose possibly the entire principal amount of the loan.  Since this number is potentially much larger of a loss, minimizing False positives should be the most important metric of our predictions.\n",
    "\n",
    "This mean we need to optimize for a high recall rate (true positive rate) and a low fallout rate (false positive rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fp = False Positive\n",
    "\n",
    "fn = False Negative\n",
    "\n",
    "tp = True Positive\n",
    "\n",
    "tn = True Negative\n",
    "\n",
    "\n",
    "False Positive Rate = fp / (fp + tn)\n",
    "\n",
    "True Positive Rate = tp / (tp + fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally speaking, as we attempt to reduce the false positive rate (loans we funded but shouldn't have), the rate of true positives will drop as well.  This makes sense, since by being more conservative about making loans in the first place, we are going to miss a few opportunities while avoiding pitfalls. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions\n",
    "\n",
    "Now that we have our data cleaned and process, or goal set, and our error metrics chosen, we can set up our algorithm and run it to see how accurately it can predict which borrowers will default on their loans before we underwrite them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Since we are making a binary prediction (will borrower pay back their loan or not), Logistic Regression is a good place to start.  It is less likely to over-fit the data as a decision tree, and less computationally intensive as a random forest.\n",
    "\n",
    "We will be using LogisticRegression from the sklearn.linear_model python library.\n",
    "We will also be performing kfold cross-validation (with five folds) to ensure that we can optimal separation of testing and training dataframes.  This should help ensure that our results hold up outside of the lab and aren't merely over-fit to the training data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing modules\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "# creating feature dataset\n",
    "features = df.drop('loan_status', axis=1)\n",
    "\n",
    "# making sure features has only 39 columns\n",
    "print(features.shape[1])\n",
    "\n",
    "# creating label out of loan status column\n",
    "label = df.loan_status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we will need to calculate our \"True Positive Rate\" and our \"False Positive Rate\" after we make our predictions, we will write a function that does it for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing function that will take out predictions from our machine learning algoirth\n",
    "# and will calculate the \"True Positive Rate\" and our \"False Positive Rate\" \n",
    "\n",
    "def tpr_and_fpr(predictions):\n",
    "    tp = (df.loan_status == 1) & (predictions == 1)   # true positives\n",
    "    fn = (df.loan_status == 1) & (predictions == 0)   # false negatives\n",
    "    fp = (df.loan_status == 0) & (predictions == 1)   # false positives\n",
    "    tn = (df.loan_status == 0) & (predictions == 0)   # true negatives\n",
    "    \n",
    "    tpr = tp.sum() / (tp.sum() + fn.sum())     # true positive rate\n",
    "    fpr = fp.sum() / (fp.sum() + tn.sum())     # false positive rate\n",
    "    \n",
    "    return tpr, fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performing Logistic Regression and making predictions\n",
    "\n",
    "# creating Logistic Regression object\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# using cross_val_predict to fit the data, perform kfold cross validation and predict\n",
    "predictions = cross_val_predict(lr, features, label, cv=5, n_jobs=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating our tpr and fpr from our predictions\n",
    "\n",
    "tpr, fpr = tpr_and_fpr(predictions)\n",
    "\n",
    "print(tpr, fpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "We can see that our Logistic Regression was hardly more useful at making predictions than simply guessing that every person would pay back their loan on time.  This is due to the large class imbalance in the data.  There is too high of a percentage of people that pay back their loans on time in the dataset for machine learning algorithms to make an accurate prediction.\n",
    "\n",
    "By weighting the rows of data where the label is '0' so that they are taken into account as much as all of the rows where the label is '1', hopefully we can get more accurate predictions.\n",
    "\n",
    "Fortunately, sklearn has a 'class_weight' parameter that balances each label in proportion to its frequency in the data set.  We can use it by setting passing the argument 'balanced' into the 'class_weight' parameter.\n",
    "\n",
    "Once we do that, since there are six times as many rows where the loan is paid off, all the rows where the loan *was not* paid off are weighted six times as strongly by the logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performing weighted Logistic Regression and making predictions\n",
    "\n",
    "# creating Logistic Regression object\n",
    "lr = LogisticRegression(class_weight='balanced')\n",
    "\n",
    "# using cross_val_predict to fit the data, perform kfold cross validation and predict\n",
    "predictions = cross_val_predict(lr, features, label, cv=5, n_jobs=8)\n",
    "\n",
    "# calculating our tpr and fpr from our predictions\n",
    "\n",
    "tpr, fpr = tpr_and_fpr(predictions)\n",
    "\n",
    "print(\"TPR: \",tpr)\n",
    "print(\"FPR: \", fpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These numbers should make the conservative investor a bit happier.  We are now writing 57% of the profitable loans, while correctly identifying 69% of the loans that would default.  To see if we can improve these numbers, we will see what happens when we manually weight the rows with '0' even higher.\n",
    "\n",
    "Currently, sklearn has the weight for a '0' label set to ~5.8x the weight for a '1' row.\n",
    "\n",
    "We will now re-run our logistic regression with the weight increased to 10x for the '0' rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performing 10x weighted Logistic Regression and making predictions\n",
    "\n",
    "# creating Logistic Regression object\n",
    "custom_weight = {0:10, 1:1}\n",
    "lr = LogisticRegression(class_weight=custom_weight)\n",
    "\n",
    "# using cross_val_predict to fit the data, perform kfold cross validation and predict\n",
    "predictions = cross_val_predict(lr, features, label, cv=5, n_jobs=8)\n",
    "\n",
    "# calculating our tpr and fpr from our predictions\n",
    "\n",
    "tpr, fpr = tpr_and_fpr(predictions)\n",
    "\n",
    "print(\"TPR: \",tpr)\n",
    "print(\"FPR: \", fpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "By weighting the columns where people defaulted heavier, we were able to eliminate 94% of the customers who would default on their loans, but we also eliminated 85% of our profitable customers, so this is probably too cautious of a model to be realistic to your average investor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regressor\n",
    "\n",
    "We could attempt to optimize our logistic regression model further, but it would be prudent to see how accurately a random forest would compare, before we dive too deeply into logistic regression.  We will now perfom the same prediction steps as before, but replacing logistic regression with random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# performing weighted Random Forest and making predictions\n",
    "\n",
    "# creating random forest object\n",
    "rf = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
    "\n",
    "# using cross_val_predict to fit the data, perform kfold cross validation and predict\n",
    "predictions = cross_val_predict(rf, features, label, cv=5, n_jobs=8)\n",
    "\n",
    "# calculating our tpr and fpr from our predictions\n",
    "\n",
    "tpr, fpr = tpr_and_fpr(predictions)\n",
    "\n",
    "print(\"TPR: \",tpr)\n",
    "print(\"FPR: \", fpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will re-run with heavier 8x weights for the '0' rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performing 8x weighted Random Forest and making predictions\n",
    "\n",
    "# creating random forest object\n",
    "custom_weight = {0:8, 1:1}\n",
    "rf = RandomForestClassifier(class_weight=custom_weight, random_state=42)\n",
    "\n",
    "# using cross_val_predict to fit the data, perform kfold cross validation and predict\n",
    "predictions = cross_val_predict(rf, features, label, cv=5, n_jobs=8)\n",
    "\n",
    "# calculating our tpr and fpr from our predictions\n",
    "\n",
    "tpr, fpr = tpr_and_fpr(predictions)\n",
    "\n",
    "print(\"TPR: \",tpr)\n",
    "print(\"FPR: \", fpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "Logistic Regression was much more accurate at predicting which customers will pay back their loans.  With more optimization, we could likely improve the accuracy.\n",
    "\n",
    "If we could link our feature table with a table that calculated the profitability of each loan (referencing the loan_id), we would be able to replace 'profitability' as the error metric instead of tpr and fpr.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
